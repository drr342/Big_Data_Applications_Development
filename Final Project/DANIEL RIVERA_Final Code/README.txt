BIG DATA APPLICATIONS DEVELOPMENT - SPRING 2019
DANIEL RIVERA RUIZ
drr342@nyu.edu

Author Profiling: Predicting Age, Gender and Personality of Twitter Users

The code for the project is divided into three main sections:
	1) Data Acquisition
	2) Modeling
	3) Visualization

1) DATA ACQUISITION
Plain scala code used to ingest tweets with the Twitter Standard Search API.
Source files:
	a) src/main/scala/search2json/age.scala - Collect tweets for the age classifier.
	b) src/main/scala/search2json/gender.scala - Collect tweets for the gender classifier.
	c) src/main/scala/search2json/person.scala - Collect tweets for the personality classifier.
	d) src/main/scala/search2json/MyTweet.scala - Auxiliary class to represent a tweet.
How to run on Dumbo Cluster:
	run.sh <age | gender | person> (depending on the tweets to be collected)
Observations:
	The collected tweets will be stored to /scratch/bdad/project/tweets/ using timestamps to avoid collisions.

2) MODELING
Spark code used to train the three classifiers (ANNs)
Source files:
	a) src/main/scala/train/train.scala - Load tweets and pass them to the three classifiers.
How to run on Dumbo Cluster:
	run.sh
Observations:
	a) All paths to data in HDFS are hardcoded in the source file.
	b) Input data is read from /user/drr342/bdad/project/tweets/
	c) Word embeddings are read from /user/drr342/bdad/project/word2vec/GoogleNews-vectors-negative300.txt
	d) Models generated and other parameters required for inference are saved to /user/drr342/bdad/project/models/
	e) Everytime a folder is generated in HDFS, its name includes a timestamp to avoid collisions.

3) VISUALIZATION
Spark code to perfome inference with pre-trained models and Python code to visualize results.
Source files:
	a) src/main/scala/test/test.scala - Gennerate predictions and top 5 similar tweets for a single tweet.
	b) src/main/scala/test/MyTweet.scala - Auxiliary class to represent the input tweet.
	c) ui/ui.py - Takes the results from the previous step and builds the user interface.
How to run on Dumbo Cluster:
	run.sh <timeStamp> <url> <savePath>
Observations:
	a) All paths to input data in HDFS are hardcoded in the source file.
	b) <timeStamp> MUST be a timestamp previously generated by the modeling stage (Valid timeStamp: 20190425_121837)
	c) <url> MUST be a URL to a valid tweet (the statusID will also work)
	d) <savePath> MUST be a path to a folder in HDFS where the results of the inference will be stored
	e) The folder from c) MUST not exist previous to the execution
	f) The script will collect the results generated by the Spark job and make them available in the local file system at ui/results.txt
	g) A sample version of the collected results is included with the submission.